{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://85aba6502a4f:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f2df48c7898>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pandas as pd, numpy as np\n",
    "import numpy as np\n",
    "from pyspark.mllib.linalg.distributed import BlockMatrix\n",
    "from pyspark.mllib.linalg import DenseMatrix, Matrices\n",
    "from pyspark.sql.functions import randn\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Generate random Matrix data and put in Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_ROWS = int(1e5)\n",
    "NUM_COLS = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(13)\n",
    "seeds = np.random.choice(range(int(1e6)),NUM_COLS,replace=False).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "sprk_df = spark.range(0,NUM_ROWS,numPartitions=100)\n",
    "\n",
    "for i in range(len(seeds)):\n",
    "    sprk_df = sprk_df.withColumn('X'+str(i+1),randn(seed=seeds[i]))\n",
    "    \n",
    "sprk_df = sprk_df.drop('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|                  X1|                 X10|                X149|\n",
      "+--------------------+--------------------+--------------------+\n",
      "| -0.7875995423413541| -1.8496231710670317| 0.14319168635324628|\n",
      "| -0.8802204696629468|  1.0227987046234572|-0.16021299654078477|\n",
      "|-0.13052186437684288| -0.6377570013161057| 0.17379403751949127|\n",
      "| -0.9819029238098963| -1.3914036254270399|-0.07337543730763679|\n",
      "| -1.1528200936909183|  0.7931504380379168| -1.1181662576458717|\n",
      "|  1.4381143086517836| -0.5923779893720164| 0.46605793183654703|\n",
      "| -0.8443907079566332|-0.14327078654999217|  1.9005297473603961|\n",
      "|  1.3409502738035992|  0.4590974616850282| -1.1233419119648724|\n",
      "|  0.7712124278965381|  1.4994883973477136|   1.614813091130632|\n",
      "|  -0.631149997027938| -0.3391861350329733| -1.8154550466148764|\n",
      "+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sprk_df.limit(10).select(['X1','X10','X149']).show(10)  #toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100,000\n"
     ]
    }
   ],
   "source": [
    "print('{:,d}'.format(sprk_df.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read matrix data from Spark Dataframe and perform X.T @ X matrix operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 150)\n",
      "(150, 150)\n",
      "time for numpy: 9.448\n"
     ]
    }
   ],
   "source": [
    "start_time = datetime.datetime.now()\n",
    "A = np.array(sprk_df.collect())\n",
    "\n",
    "print(A.shape)\n",
    "\n",
    "B = np.dot(A.T, A)\n",
    "\n",
    "print(B.shape)\n",
    "print('time for numpy: {:.3f}'.format((datetime.datetime.now() - start_time).total_seconds()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create BlockMatrix from Spark dataframe and perform X.T @ X matrix operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.datetime.now()\n",
    "rdd= sprk_df.rdd.map(list)\n",
    "COLUMNS = len(rdd.take(1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create closure for partitioning function to consolidate single row dense matrix into larger blocks\n",
    "# Based on number of dense matrices and specified rows per block, calculate number of blocks to make up the \n",
    "# block matrix and return a partition function to be used in partitionBy() function\n",
    "\n",
    "def createPartitionFunction(num_items,block_rows):\n",
    "    # Calculate number of blocks that will make up the BlockMatrix\n",
    "    number_of_blocks = int(np.ceil(num_items/block_rows))\n",
    "    \n",
    "    # return number of blocks and closure to be used by partitionBy() function\n",
    "    return number_of_blocks, lambda k:int(k/block_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROWS_PER_BLOCK = 1600\n",
    "number_of_blocks, partition_function = createPartitionFunction(rdd.count(),ROWS_PER_BLOCK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to stack single row dense matrices into larger block based on partitionBy()\n",
    "\n",
    "def stackDenseMatrices(iter_x):\n",
    "    # convert each dense matrix in this partition to an element in a list\n",
    "    vector_list = [dm[1].toArray() for dm in iter_x]\n",
    "    \n",
    "    # stack all the elements in the list to a single array\n",
    "    combined_array = np.vstack(vector_list)\n",
    "\n",
    "    # convert the single array to a larger dense matrix\n",
    "    values = [x for x in combined_array.flatten('F').tolist()]\n",
    "    den_mat = DenseMatrix(combined_array.shape[0],combined_array.shape[1],\n",
    "                          values)\n",
    "    \n",
    "    # return larger dense matrix\n",
    "    return [den_mat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert rdd of single row dense matrices to rdd of larger dense matrices to form the BlockMatrix\n",
    "\n",
    "# create key-value rdd with index value of single-row dense matrix as key\n",
    "rdd2 = rdd.map(lambda x: DenseMatrix(1,COLUMNS,x)).zipWithIndex().map(lambda x:(x[1],x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repartition rdd such that all single-row dense matrices that make up a block are in a single partition\n",
    "rdd2 = rdd2.partitionBy(number_of_blocks,partition_function)\n",
    "\n",
    "\n",
    "# Based on number of blocks, addd partition number to key and ensure within partition rows are in order\n",
    "def addPartitionIdToKey(x):\n",
    "    partition_id = x[1]\n",
    "    dm_list = [((partition_id,y[0]),y[1]) for y in x[0]]\n",
    "    return dm_list\n",
    "    \n",
    "rdd2 = rdd2.mapPartitions(lambda x:[list(x)]).zipWithIndex().map(addPartitionIdToKey).flatMap(lambda x:x)\n",
    "\n",
    "# create partitions by block and ensure rows are sorted correctly\n",
    "rdd2 =  rdd2.repartitionAndSortWithinPartitions(number_of_blocks,lambda k:k[0])\n",
    "\n",
    "\n",
    "# stack all single-row dense matrices in a partition into one larger dense matrix\n",
    "rdd2 = rdd2.mapPartitions(stackDenseMatrices)\n",
    "\n",
    "# create rdd of block matrix structure\n",
    "rdd2 = rdd2.zipWithIndex().map(lambda x:((x[1],0),x[0]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "Block_A = BlockMatrix(rdd2,ROWS_PER_BLOCK,COLUMNS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "Block_B = Block_A.transpose().multiply(Block_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time for BlockMatrix: 42.136\n"
     ]
    }
   ],
   "source": [
    "B2 = Block_B.toLocalMatrix().toArray()\n",
    "print('time for BlockMatrix: {:.3f}'.format((datetime.datetime.now() - start_time).total_seconds()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare BlockMatrix answer with Numpy Answer and calculate relative error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array_equal(B,B2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0186340659856796e-10"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(np.abs(B2-B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relative Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.2590965251122286e-11"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(np.abs((B2-B)/B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
